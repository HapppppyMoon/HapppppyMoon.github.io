---
title: "[ì—°êµ¬ì¼ì§€] 2024ë…„ 1ì›” 4ì£¼ì°¨ - RL ê¸°ë°˜ ê¶¤ì  ìµœì í™”"
date: 2025-09-29
categories: projects
tags: [research-log, reinforcement-learning, trajectory-planning]
toc: true
toc_sticky: true
published: false
---

## ğŸ“… ì´ë²ˆ ì£¼ ìš”ì•½

ì´ë²ˆ ì£¼ëŠ” ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì˜ í•™ìŠµ ì•ˆì •ì„±ì„ ê°œì„ í•˜ëŠ”ë° ì§‘ì¤‘í–ˆë‹¤.
PPO ì•Œê³ ë¦¬ì¦˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ reward shapingì„ í†µí•´ ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í–ˆë‹¤.

## ğŸ¯ ì£¼ê°„ ëª©í‘œ

- [x] PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- [x] Reward function ì¬ì„¤ê³„
- [ ] Sim-to-real transfer ì‹¤í—˜ ì¤€ë¹„
- [ ] ë² ì´ìŠ¤ë¼ì¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµ ì‹¤í—˜

## ğŸ“Š ì‹¤í—˜ ê²°ê³¼

### í•™ìŠµ ê³¡ì„ 

```
Episode 1000: Average Reward = 124.5 Â± 12.3
Episode 2000: Average Reward = 256.8 Â± 8.7
Episode 3000: Average Reward = 312.4 Â± 5.2
```

### ì£¼ìš” ë°œê²¬ì‚¬í•­

1. **Learning rate scheduling**ì´ ìˆ˜ë ´ ì†ë„ì— í° ì˜í–¥
2. Entropy coefficientë¥¼ 0.01ì—ì„œ 0.005ë¡œ ë‚®ì¶˜ í›„ policy ì•ˆì •ì„± í–¥ìƒ
3. Reward clippingì´ ì˜¤íˆë ¤ ì„±ëŠ¥ ì €í•˜ ìœ ë°œ

## ğŸ’¡ ì•„ì´ë””ì–´ & ì¸ì‚¬ì´íŠ¸

### Curiosity-driven Exploration
- í˜„ì¬ epsilon-greedy ë°©ì‹ì˜ í•œê³„ì  ë°œê²¬
- Intrinsic motivation ê¸°ë°˜ exploration ë°©ë²• ê³ ë ¤ ì¤‘
- ì°¸ê³  ë…¼ë¬¸: [Curiosity-driven Exploration by Self-supervised Prediction](https://arxiv.org/abs/1705.05363)

### Multi-objective Optimization
- ë‹¨ì¼ reward functionì˜ í•œê³„
- Pareto optimal solution íƒìƒ‰ í•„ìš”
- TODO: Multi-objective RL ë…¼ë¬¸ ë¦¬ë·°

## ğŸ› ì´ìŠˆ & í•´ê²°

### Issue #1: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
**ë¬¸ì œ**: Batch size 512ì—ì„œ OOM ì—ëŸ¬ ë°œìƒ
**ì›ì¸**: Replay bufferê°€ GPU ë©”ëª¨ë¦¬ì— ìƒì£¼
**í•´ê²°**:
```python
# CPUë¡œ replay buffer ì´ë™
self.replay_buffer = ReplayBuffer(size=100000, device='cpu')
# í•™ìŠµ ì‹œì—ë§Œ GPUë¡œ ì´ë™
batch = batch.to('cuda')
```

### Issue #2: í•™ìŠµ ë¶ˆì•ˆì •ì„±
**ë¬¸ì œ**: 2000 episode ì´í›„ rewardê°€ ê¸‰ê²©íˆ ê°ì†Œ
**ì›ì¸**: Learning rateê°€ ë„ˆë¬´ ë†’ìŒ (1e-3)
**í•´ê²°**: Cosine annealing scheduler ì ìš©

## ğŸ“ ì½”ë“œ ìŠ¤ë‹ˆí«

### Reward Shaping í•¨ìˆ˜
```python
def compute_reward(self, state, action, next_state):
    # Distance to goal
    dist_reward = -np.linalg.norm(next_state[:3] - self.goal)

    # Smoothness penalty
    jerk = np.diff(action, n=2)
    smooth_penalty = -0.1 * np.sum(jerk**2)

    # Energy efficiency
    energy_penalty = -0.05 * np.sum(action**2)

    return dist_reward + smooth_penalty + energy_penalty
```

## ğŸ”„ ë‹¤ìŒ ì£¼ ê³„íš

1. **Sim-to-real Transfer**
   - Domain randomization íŒŒë¼ë¯¸í„° ì„¤ì •
   - Real robot í™˜ê²½ ì…‹ì—…
   - Safety constraints êµ¬í˜„

2. **ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ**
   - RRT* ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
   - MPC (Model Predictive Control) ë¹„êµ
   - ê³„ì‚° ì‹œê°„ ë° ìµœì ì„± ë¶„ì„

3. **ë…¼ë¬¸ ì‘ì„±**
   - Related work ì„¹ì…˜ ì´ˆì•ˆ ì‘ì„±
   - ì‹¤í—˜ ê²°ê³¼ ì •ë¦¬

## ğŸ“š ì°¸ê³  ìë£Œ

- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [Sim-to-Real: Learning Agile Locomotion](https://arxiv.org/abs/1804.10332)
- [ROS2 RL Tutorial](https://docs.ros.org/en/humble/Tutorials/Reinforcement-Learning/)

## ğŸ’¬ ë©”ëª¨

- ë‚´ì¼ êµìˆ˜ë‹˜ ë¯¸íŒ…ì—ì„œ domain randomization ë²”ìœ„ ë…¼ì˜ í•„ìš”
- Lab seminar ë°œí‘œ ì¤€ë¹„ (2/1)
- ICRA 2025 deadline: 2024/09/15

---

*ì—°êµ¬ ì¼ì§€ëŠ” ë§¤ì£¼ ê¸ˆìš”ì¼ì— ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.*
